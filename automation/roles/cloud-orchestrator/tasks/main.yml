- name: Create a VPC
  become: false
  environment: "{{ env_vars }}"
  amazon.aws.ec2_vpc_net:
    name: datapipeline_vpc
    cidr_block: 10.10.0.0/16
    region: us-east-1
    state: present
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
  register: datapipeline_vpc

- name: Create an Internet Gateway and attach to VPC
  become: false
  environment: "{{ env_vars}} "
  community.aws.ec2_vpc_igw:
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
    vpc_id: "{{ datapipeline_vpc.vpc.id }}"
    state: present
    region: us-east-1
    tags:
      app: datapipeline
  register: datapipeline_internet_gateway

- name: Create a VPC Subnet
  become: false
  environment: "{{ env_vars }}"
  amazon.aws.ec2_vpc_subnet:
    cidr: 10.10.2.0/24
    region: us-east-1
    vpc_id: "{{ datapipeline_vpc.vpc.id }}"
    state: present
    tags:
      name: datapipeline_vpc_subnet
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
  register: datapipeline_vpc_subnet

- name: Create a public route table for VPC Subnet to allow internet access
  become: false
  environment: "{{ env_vars }}"
  community.aws.ec2_vpc_route_table:
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
    vpc_id: "{{ datapipeline_vpc.vpc.id }}"
    region: us-east-1
    state: present
    tags:
      Name: datapipeline_public_route_table
    subnets:
      - "{{ datapipeline_vpc_subnet.subnet.id }}"
    routes:
      - dest: 0.0.0.0/0
        gateway_id: "{{ datapipeline_internet_gateway.gateway_id }}"
  register: public_route_table

- name: Create a security group to allow host reachability
  become: false
  environment: "{{ env_vars }}"
  amazon.aws.ec2_group:
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
    description: security group for kafka, epmd, couchdb, zookeeper, ssh, k8s
    name: kafka-couchdb-zookeeper-k8s-security
    vpc_id: "{{ datapipeline_vpc.vpc.id }}"
    rules:
      - proto: tcp
        ports:
        - 22
        cidr_ip: 0.0.0.0/0
        rule_desc: ssh
      - proto: udp
        ports:
        - 53
        cidr_ip: 0.0.0.0/0
        rule_desc: DNS port for kubernetes (UDP)
      - proto: tcp
        ports:
        - 53
        cidr_ip: 0.0.0.0/0
        rule_desc: DNS port for kubernetes (TCP)
      - proto: tcp
        ports:
        - 9153
        cidr_ip: 0.0.0.0/0
        rule_desc: metrics CoreDNS port for kubernetes
      - proto: tcp
        ports:
        - 2181
        cidr_ip: 0.0.0.0/0
        rule_desc: zookeeper
      - proto: tcp
        ports:
        - 4369
        cidr_ip: 0.0.0.0/0
        rule_desc: erlang port mapper daemon (epmd)
      - proto: tcp
        ports:
        - 5984
        cidr_ip: 0.0.0.0/0
        rule_desc: couchdb
      - proto: tcp
        ports:
        - 9092
        cidr_ip: 0.0.0.0/0
        rule_desc: apache kafka
      # reference on k8s port https://docs.oracle.com/en/operating-systems/olcne/1.1/start/ports.html
      - proto: tcp
        ports:
        - 2379
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes etcd server client API (on master nodes in multi-master deployments)
      - proto: tcp
        ports:
        - 2380
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes etcd server client API (on master nodes in multi-master deployments)
      - proto: tcp
        ports:
        - 6443
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes API server (master nodes)
      - proto: tcp
        ports:
        - 8090
        cidr_ip: 0.0.0.0/0
        rule_desc: Platform Agent (master and worker nodes)
      - proto: tcp
        ports:
        - 8091
        cidr_ip: 0.0.0.0/0
        rule_desc: Platform API Server (operator node)
      - proto: udp
        ports:
        - 8472
        cidr_ip: 0.0.0.0/0
        rule_desc: Flannel overlay network, VxLAN backend (master and worker nodes)
      - proto: tcp
        ports:
        - 10250
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes kubelet API server (master and worker nodes)
      - proto: tcp
        ports:
        - 10251
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes kube-scheduler (on master nodes in multi-master deployments)
      - proto: tcp
        ports:
        - 10252
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes kube-controller-manager (on master nodes in multi-master deployments)
      - proto: tcp
        ports:
        - 10255
        cidr_ip: 0.0.0.0/0
        rule_desc: Kubernetes kubelet API server for read-only access with no authentication (master and worker nodes)
      # additional reference https://www.cloudiqtech.com/creating-aws-security-groups-for-kubernetes/
      - proto: tcp
        ports:
        - 443
        cidr_ip: 0.0.0.0/0
        rule_desc: Secure Kubernetes API server (master nodes)
      - proto: tcp
        ports:
        - 8080
        cidr_ip: 0.0.0.0/0
        rule_desc: Container Network Virtualization
      - proto: tcp
        ports:
        - 30984
        cidr_ip: 0.0.0.0/0
        rule_desc: NodePort for couchdb Kubernetes Service for public IP GUI access
      - proto: tcp
        ports:
        - 30985
        cidr_ip: 0.0.0.0/0
        rule_desc: NodePort for Kafka Kubernetes Service for public bootstrap access
      - proto: tcp
        ports:
        - 7077
        - 8080
        - 30008
        cidr_ip: 0.0.0.0/0
        rule_desc: Apache spark master
      - proto: tcp
        ports:
        - 7076
        cidr_ip: 0.0.0.0/0
        rule_desc: Apache spark driver
      - proto: tcp
        ports:
        - 7078
        - 8081
        cidr_ip: 0.0.0.0/0
        rule_desc: Apache spark worker
      - proto: tcp
        ports:
        - 7079
        cidr_ip: 0.0.0.0/0
        rule_desc: Apache spark block manager


    state: present
    region: us-east-1

- name: Create EC2 Instance 1
  become: false
  environment: "{{ env_vars }}"
  community.aws.ec2_instance:
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
    # when I initially tried it, with the original settings from austin, it will identify some existing instance
    # then it will try to use the existing instance, since I terminate these old instance (the one created in prior runs)
    # it will throw out errors, not exactly sure how these filters will work, but have to be careful with these
    filters:
      "tag:app": datapipeline-a3
      "tag:Name": 5287cloudvm0-a3
    name: 5287cloudvm0-a3
    tags:
      # Add tags so you can start / stop by tags. Running twice should not create duplicates.
      app: datapipeline-a3
    key_name: aws-keypair3
    instance_type: t2.medium
    vpc_subnet_id: "{{ datapipeline_vpc_subnet.subnet.id }}"
    network:
      assign_public_ip: yes
      delete_on_termination: yes
    region: us-east-1
    # Ubuntu 20.04 LTS AMI
    image_id: ami-09e67e426f25ce0d7
    state: running
    # Wait for the instances to reach their desired states before returning.
    # Does not wait for SSH, see the 'wait_for_connection' example for details.
    wait: yes
    # Security group to set up appropriate firewall rules
    security_group: kafka-couchdb-zookeeper-k8s-security
  register: ec2_1

- name: Create EC2 Instance 2
  become: false
  environment: "{{ env_vars }}"
  community.aws.ec2_instance:
    aws_access_key: "{{ aws_access_key_id.stdout }}"
    aws_secret_key: "{{ aws_secret_access_key.stdout }}"
    filters:
      "tag:app": datapipeline-a3
      "tag:Name": 5287cloudvm1-a3
    name: 5287cloudvm1-a3
    tags:
      # Add tags so you can start / stop by tags. Running twice should not create duplicates.
      app: datapipeline-a3
    key_name: aws-keypair3
    instance_type: t2.medium
    vpc_subnet_id: "{{ datapipeline_vpc_subnet.subnet.id }}"
    network:
      assign_public_ip: yes
      delete_on_termination: yes
    region: us-east-1
    # Ubuntu 20.04 LTS AMI
    image_id: ami-09e67e426f25ce0d7
    state: running
    # Wait for the instances to reach their desired states before returning.
    # Does not wait for SSH, see the 'wait_for_connection' example for details.
    wait: yes
    # Security group to set up appropriate firewall rules
    security_group: kafka-couchdb-zookeeper-k8s-security
  register: ec2_2

- name: Wait for SSH to come up on VM 1 # SSH needs to work for rest of playbook to continue
  wait_for:
    host: "{{ item.public_ip_address }}"
    port: 22
    state: started
  with_items: "{{ ec2_1.instances }}"

- name: Wait for SSH to come up on VM 2 # SSH needs to work for rest of playbook to continue
  wait_for:
    host: "{{ item.public_ip_address }}"
    port: 22
    state: started
  with_items: "{{ ec2_2.instances }}"

- name: Add EC2 instance 1 to groups cloud and k8smaster
  add_host:
    name: "{{ item.public_ip_address }}"
    ansible_ssh_private_key_file: "{{ playbook_dir }}/.ssh/aws-keypair3.pem"
    groups:
      - cloud
      - k8smaster
  with_items: "{{ ec2_1.instances }}"

- name: Add EC2 instance 2 to groups cloud and k8sworker
  add_host:
    name: "{{ item.public_ip_address }}"
    ansible_ssh_private_key_file: "{{ playbook_dir }}/.ssh/aws-keypair3.pem"
    groups:
      - cloud
      - k8sworker
  with_items: "{{ ec2_2.instances }}"